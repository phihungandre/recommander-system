{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ[\"HADOOP_HOME\"] = \"\"\n",
    "os.environ[\"hadoop.home.dir\"] = \"\"\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Set plot parameters\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 13)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concernant la méthode abordé pour le projet final, j'ai décidé de prendre comme méthode la méthode ALS (Alternating Least Squares) appliqué en cours.\n",
    "\n",
    "Dans l'idée, comme cité dans le cours, l’algorithme ALS (Alternating Least Squares) vise à minimiser la même fonction objective d’erreur au carré que FunkSVD, mais aborde le problème de minimisation en utilisant une méthode de moindres carrés alternée.\n",
    "L’algorithme fonctionne en fixant d’abord la matrice user et en minimisant la fonction objective par rapport à la matrice item(dans notre cas vendor), puis en fixant la matrice item et en minimisant la fonction objective par rapport à la matrice user. Ce processus est répété jusqu’à convergence. À chaque itération, l’algorithme résout un problème de moindres carrés pour mettre à jour la matrice user ou item.\n",
    "\n",
    "De ce fait, il a fallut que je fasse en premier lieu un preprocessing des données pour pouvoir appliquer la méthode qui m'était convenu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse des données Train Customers\n",
    "Dans un premier temps, j'ai décidé de faire une analyse des données pour pouvoir comprendre les données que j'avais à disposition.\n",
    "En effet, c'est un point important pour pouvoir faire un bon preprocessing des données et pouvoir discerner les données utiles des autres données mais aussi de pouvoir compléter les données manquantes.\n",
    "Dans les données récupérées, j'ai décidé de garder les colonnes suivantes qui me semblaient utiles pour la suite :\n",
    "- gender : comprend les valeurs M, F, ? et vide donc il faudra faire un traitement pour ne garder que M et F\n",
    "- verified : comprend les valeurs 0 et 1 donc il faudra faire un traitement pour ne garder que 1\n",
    "- created_at : comprend des dates donc il faudra faire un traitement pour pouvoir les utiliser\n",
    "- updated_at : comprend des dates donc il faudra faire un traitement pour pouvoir les utiliser\n",
    "- customer_id : comprend des identifiants donc il faudra faire un traitement pour pouvoir les utiliser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_training = pd.read_csv('./data/train_customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_users_training.describe(include=\"all\")\n",
    "#df_users_training.info(memory_usage=\"deep\")\n",
    "df_users_training[\"created_at\"] = pd.to_datetime(df_users_training[\"created_at\"])\n",
    "df_users_training[\"updated_at\"] = pd.to_datetime(df_users_training[\"updated_at\"])\n",
    "df_users_training.rename(columns={'akeed_customer_id': 'customer_id'}, inplace=True)\n",
    "\n",
    "# Drop useless columns\n",
    "df_users_training.drop(['language', 'dob', 'created_at'], axis=1, inplace=True)\n",
    "\n",
    "#make gender only two values\n",
    "df_users_training.gender.replace(to_replace=[r'[F|f].*', r'[M|m|?| ].*'], value=['F', 'M'], inplace=True, regex=True)\n",
    "\n",
    "#take only verified users\n",
    "df_users_training = df_users_training[df_users_training.verified == 1]\n",
    "\n",
    "# drop verified column and status column\n",
    "df_users_training.drop(['verified'], axis=1, inplace=True)\n",
    "df_users_training.drop(['status'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse des données Train Locations\n",
    "Dans un second temps, il m'a fallut faire une analyse des données de la table Locations pour pouvoir faire un preprocessing des données.\n",
    "Le but étant de rejoindre toutes les données entre elles et de pouvoir faire des prédictions sur les données manquantes.\n",
    "Dans les données récupérées, j'ai décidé de garder les colonnes suivantes qui me semblaient utiles pour la suite :\n",
    "- location_number : comprend des identifiants donc il faudra faire un traitement pour pouvoir les utiliser\n",
    "- location_type : comprend des valeurs comme Home, Work, Other, etc. donc il faudra faire un traitement pour pouvoir les utiliser à savoir faire en sorte que les NaN deviennent Other\n",
    "- latitude : correspond à la latitude comme son nom l'indique\n",
    "- longitude : correspond à la longitude comme son nom l'indique\n",
    "- customer_id : comprend des identifiants qui permettra de faire la jointure avec la table Customers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_users_loc_training = pd.read_csv('./data/train_locations.csv')\n",
    "#df_users_loc_training.describe(include=\"all\")\n",
    "#df_users_loc_training.info(memory_usage=\"deep\")\n",
    "# fill missing values from location_type\n",
    "df_users_loc_training.location_type.fillna('Other', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse des données Train Orders\n",
    "Dans un troisième temps, il a été d'une importance d'analyser les datas d'order puisque la colonne de distance nous permettra de pouvoir faire un rating.\n",
    "Bien entendu, il a fallut faire un preprocessing des données pour pouvoir les utiliser et le fusionner avec les autres tables par la suite."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phi-h\\AppData\\Local\\Temp\\ipykernel_55356\\1648026616.py:1: DtypeWarning: Columns (15,16,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_orders = pd.read_csv('./data/orders.csv')\n"
     ]
    }
   ],
   "source": [
    "df_orders = pd.read_csv('./data/orders.csv')\n",
    "#df_orders.describe(include='all')\n",
    "#df_orders.info(memory_usage=\"deep\")\n",
    "# lier avec customer_id, location_number et location_type\n",
    "df_orders.promo_code=df_orders.promo_code.fillna('', inplace=True)\n",
    "df_orders.promo_code=df_orders.promo_code.astype('bool')\n",
    "df_orders.promo_code_discount_percentage.fillna(0, inplace=True)\n",
    "df_orders.LOCATION_TYPE.fillna('Other', inplace=True)\n",
    "df_orders.drop(['delivery_time'], axis=1, inplace=True)\n",
    "df_orders.rename(columns={'LOCATION_TYPE': 'location_type'}, inplace=True)\n",
    "df_orders.rename(columns={'LOCATION_NUMBER': 'location_number'}, inplace=True)\n",
    "# Supprimer les lignes sans order_id\n",
    "# promo code important ? ou présence d'un promo code important\n",
    "# promo_code_discount_percentage > promo_code : promo_code_discount_percentage peut être à 0\n",
    "df_orders.is_favorite.fillna('', inplace=True)\n",
    "df_orders.is_favorite = df_orders.is_favorite.astype('bool')\n",
    "df_orders.is_favorite = df_orders.is_favorite.replace({True: 1, False: 0})\n",
    "df_orders.promo_code.fillna('', inplace=True)\n",
    "df_orders.promo_code = df_orders.promo_code.astype('bool')\n",
    "df_orders.promo_code = df_orders.promo_code.replace({True: 1, False: 0})\n",
    "df_orders.is_rated = df_orders.is_rated.replace({'No': 0, 'Yes': 1})\n",
    "df_orders.akeed_order_id = df_orders.akeed_order_id.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df_users_training = pd.merge(df_users_training, df_users_loc_training, on='customer_id', how='left')\n",
    "df_users_training = df_users_training.dropna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_merge_training = pd.merge(df_users_training, df_orders, on=['customer_id', 'location_number', 'location_type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comme dis précédemment, nous utilisons la colonne de distance pour pouvoir faire un rating.\n",
    "le rating est calculé de la manière suivante :\n",
    "- Normaliser la distance entre 0 et 1\n",
    "- Convertir la distance normalisée en note\n",
    "- La note est comprise entre 1 et 5\n",
    "- Plus la distance est grande, plus la note est petite\n",
    "- Plus la distance est petite, plus la note est grande\n",
    "- La note est inversement proportionnelle à la distance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def distance_to_rating(distance, min_distance, max_distance, min_rating, max_rating):\n",
    "    # Normaliser la distance entre 0 et 1\n",
    "    normalized_distance = (distance - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "    # Convertir la distance normalisée en note\n",
    "    rating = normalized_distance * (max_rating - min_rating) + min_rating\n",
    "\n",
    "    return rating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "distances = np.array([1.0, 2.0, 3.0])\n",
    "min_distance = 1.0\n",
    "max_distance = 3.0\n",
    "min_rating = 1.0 # Inverser l'ordre des notes\n",
    "max_rating = 5.0 # Inverser l'ordre des notes\n",
    "#create a column in df_merge_training with the rating\n",
    "df_merge_training['rating'] = distance_to_rating(df_merge_training.deliverydistance, df_merge_training.deliverydistance.min(), df_merge_training.deliverydistance.max(), min_rating, max_rating)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "      CID X LOC_NUM X VENDOR  vendor    rating\n0          TCHWPBT X 0 X 237     237  1.000000\n1          TCHWPBT X 2 X 113     113  1.000000\n2          ZGFSYCZ X 0 X 303     303  1.000000\n7          ZGFSYCZ X 0 X 274     274  1.000000\n8           ZGFSYCZ X 1 X 33      33  1.000000\n...                      ...     ...       ...\n77850      U2OTA4O X 0 X 573     573  1.810783\n77851      Z7RQ368 X 0 X 160     160  2.797823\n77852      WIIU12E X 0 X 573     573  1.501814\n77853       LE63M0S X 0 X 84      84  1.640747\n77854      8PSO92C X 0 X 237     237  1.219803\n\n[49989 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CID X LOC_NUM X VENDOR</th>\n      <th>vendor</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TCHWPBT X 0 X 237</td>\n      <td>237</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TCHWPBT X 2 X 113</td>\n      <td>113</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ZGFSYCZ X 0 X 303</td>\n      <td>303</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ZGFSYCZ X 0 X 274</td>\n      <td>274</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ZGFSYCZ X 1 X 33</td>\n      <td>33</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>77850</th>\n      <td>U2OTA4O X 0 X 573</td>\n      <td>573</td>\n      <td>1.810783</td>\n    </tr>\n    <tr>\n      <th>77851</th>\n      <td>Z7RQ368 X 0 X 160</td>\n      <td>160</td>\n      <td>2.797823</td>\n    </tr>\n    <tr>\n      <th>77852</th>\n      <td>WIIU12E X 0 X 573</td>\n      <td>573</td>\n      <td>1.501814</td>\n    </tr>\n    <tr>\n      <th>77853</th>\n      <td>LE63M0S X 0 X 84</td>\n      <td>84</td>\n      <td>1.640747</td>\n    </tr>\n    <tr>\n      <th>77854</th>\n      <td>8PSO92C X 0 X 237</td>\n      <td>237</td>\n      <td>1.219803</td>\n    </tr>\n  </tbody>\n</table>\n<p>49989 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matrix_train = df_merge_training[['customer_id', 'location_number', 'vendor_id', 'rating']]\n",
    "df_matrix_train = df_matrix_train.assign(C=lambda x: x['customer_id'].astype(str) + ' X ' + x['location_number'].astype(str) + ' X ' + x['vendor_id'].astype(str))\n",
    "df_matrix_train.rename(columns={'C': 'CID X LOC_NUM X VENDOR', 'vendor_id': 'vendor'}, inplace=True)\n",
    "#remove duplicates\n",
    "df_matrix_train = df_matrix_train.drop_duplicates(subset=['CID X LOC_NUM X VENDOR'])\n",
    "df_matrix_train = df_matrix_train[['CID X LOC_NUM X VENDOR', 'vendor', 'rating']]\n",
    "df_matrix_train = df_matrix_train.dropna()\n",
    "df_matrix_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conversion de df_train en dataframe spark pour l'ALS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+------------------+\n",
      "|CID X LOC_NUM X VENDOR|vendor|            rating|\n",
      "+----------------------+------+------------------+\n",
      "|     TCHWPBT X 0 X 237|   237|               1.0|\n",
      "|     TCHWPBT X 2 X 113|   113|               1.0|\n",
      "|     ZGFSYCZ X 0 X 303|   303|               1.0|\n",
      "|     ZGFSYCZ X 0 X 274|   274|               1.0|\n",
      "|      ZGFSYCZ X 1 X 33|    33|               1.0|\n",
      "|      ZGFSYCZ X 1 X 28|    28|               1.0|\n",
      "|     ZGFSYCZ X 1 X 310|   310|               1.0|\n",
      "|     ZGFSYCZ X 3 X 300|   300|               1.0|\n",
      "|     ZGFSYCZ X 3 X 159|   159|               1.0|\n",
      "|      ZGFSYCZ X 4 X 33|    33|               1.0|\n",
      "|     ZGFSYCZ X 5 X 221|   221|               1.0|\n",
      "|      ZGFSYCZ X 5 X 33|    33|               1.0|\n",
      "|     ZGFSYCZ X 5 X 303|   303|               1.0|\n",
      "|    ZGFSYCZ X 11 X 356|   356|               1.0|\n",
      "|    ZGFSYCZ X 11 X 845|   845|               1.0|\n",
      "|    ZGFSYCZ X 12 X 582|   582|1.1099015033696216|\n",
      "|    ZGFSYCZ X 13 X 676|   676|2.4328667703473306|\n",
      "|      S2ALZFL X 0 X 78|    78|               1.0|\n",
      "|     S2ALZFL X 1 X 113|   113|               1.0|\n",
      "|      S2ALZFL X 2 X 78|    78|               1.0|\n",
      "+----------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ALSMatrixFactorisation\").config(\"spark.executor.memory\", \"16g\").config(\"spark.driver.memory\",\"16g\").getOrCreate()\n",
    "df_training = spark.createDataFrame(df_matrix_train)\n",
    "df_training.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+------------------+------------+----------------------------+\n",
      "|CID X LOC_NUM X VENDOR|vendor|            rating|vendor_index|CID X LOC_NUM X VENDOR_index|\n",
      "+----------------------+------+------------------+------------+----------------------------+\n",
      "|     TCHWPBT X 0 X 237|   237|               1.0|        53.0|                     40582.0|\n",
      "|     TCHWPBT X 2 X 113|   113|               1.0|         0.0|                     40583.0|\n",
      "|     ZGFSYCZ X 0 X 303|   303|               1.0|        85.0|                     49174.0|\n",
      "|     ZGFSYCZ X 0 X 274|   274|               1.0|        63.0|                     49173.0|\n",
      "|      ZGFSYCZ X 1 X 33|    33|               1.0|        22.0|                     49177.0|\n",
      "|      ZGFSYCZ X 1 X 28|    28|               1.0|        15.0|                     49175.0|\n",
      "|     ZGFSYCZ X 1 X 310|   310|               1.0|        74.0|                     49176.0|\n",
      "|     ZGFSYCZ X 3 X 300|   300|               1.0|        67.0|                     49183.0|\n",
      "|     ZGFSYCZ X 3 X 159|   159|               1.0|        17.0|                     49182.0|\n",
      "|      ZGFSYCZ X 4 X 33|    33|               1.0|        22.0|                     49184.0|\n",
      "|     ZGFSYCZ X 5 X 221|   221|               1.0|        45.0|                     49185.0|\n",
      "|      ZGFSYCZ X 5 X 33|    33|               1.0|        22.0|                     49187.0|\n",
      "|     ZGFSYCZ X 5 X 303|   303|               1.0|        85.0|                     49186.0|\n",
      "|    ZGFSYCZ X 11 X 356|   356|               1.0|        19.0|                     49178.0|\n",
      "|    ZGFSYCZ X 11 X 845|   845|               1.0|        90.0|                     49179.0|\n",
      "|    ZGFSYCZ X 12 X 582|   582|1.1099015033696216|        82.0|                     49180.0|\n",
      "|    ZGFSYCZ X 13 X 676|   676|2.4328667703473306|        78.0|                     49181.0|\n",
      "|      S2ALZFL X 0 X 78|    78|               1.0|         3.0|                     39051.0|\n",
      "|     S2ALZFL X 1 X 113|   113|               1.0|         0.0|                     39052.0|\n",
      "|      S2ALZFL X 2 X 78|    78|               1.0|         3.0|                     39053.0|\n",
      "+----------------------+------+------------------+------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(df_training.columns)-set(['rating'])) ]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "df_training = pipeline.fit(df_training).transform(df_training)\n",
    "df_training.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse des données Test Customers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9768 entries, 0 to 9767\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   akeed_customer_id  9768 non-null   object \n",
      " 1   gender             6321 non-null   object \n",
      " 2   dob                848 non-null    float64\n",
      " 3   status             9768 non-null   int64  \n",
      " 4   verified           9768 non-null   int64  \n",
      " 5   language           5928 non-null   object \n",
      " 6   created_at         9768 non-null   object \n",
      " 7   updated_at         9768 non-null   object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_users_test = pd.read_csv('./data/test_customers.csv')\n",
    "df_users_test.describe(include=\"all\")\n",
    "df_users_test.info(memory_usage=\"deep\")\n",
    "df_users_test[\"created_at\"] = pd.to_datetime(df_users_test[\"created_at\"])\n",
    "df_users_test[\"updated_at\"] = pd.to_datetime(df_users_test[\"updated_at\"])\n",
    "df_users_test.rename(columns={'akeed_customer_id': 'customer_id'}, inplace=True)\n",
    "\n",
    "# Drop useless columns\n",
    "df_users_test.drop(['language', 'dob', 'created_at'], axis=1, inplace=True)\n",
    "\n",
    "#make gender only two values\n",
    "df_users_test.gender.replace(to_replace=[r'[F|f].*', r'[M|m|?| ].*'], value=['F', 'M'], inplace=True, regex=True)\n",
    "\n",
    "#take only verified users\n",
    "df_users_test = df_users_test[df_users_test.verified == 1]\n",
    "\n",
    "\n",
    "# drop verified column and status column\n",
    "#df_users_test.drop(['verified'], axis=1, inplace=True)\n",
    "#df_users_test.drop(['status'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse des données Test Locations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16720 entries, 0 to 16719\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   customer_id      16720 non-null  object \n",
      " 1   location_number  16720 non-null  int64  \n",
      " 2   location_type    9070 non-null   object \n",
      " 3   latitude         16717 non-null  float64\n",
      " 4   longitude        16717 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_users_loc_test = pd.read_csv('./data/test_locations.csv')\n",
    "df_users_loc_test.describe(include=\"all\")\n",
    "df_users_loc_test.info(memory_usage=\"deep\")\n",
    "# fill missing values from location_type\n",
    "df_users_loc_test.location_type.fillna('Other', inplace=True)\n",
    "#df_users_loc_test.location_type.fillna('Other', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16720 entries, 0 to 16719\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   customer_id      16720 non-null  object        \n",
      " 1   gender           11348 non-null  object        \n",
      " 2   status           15814 non-null  float64       \n",
      " 3   verified         15814 non-null  float64       \n",
      " 4   updated_at       15814 non-null  datetime64[ns]\n",
      " 5   location_number  16720 non-null  int64         \n",
      " 6   location_type    16720 non-null  object        \n",
      " 7   latitude_user    16717 non-null  float64       \n",
      " 8   longitude_user   16717 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), object(3)\n",
      "memory usage: 3.6 MB\n"
     ]
    }
   ],
   "source": [
    "df_users_test = pd.merge(df_users_test, df_users_loc_test, on='customer_id', how='right')\n",
    "df_users_test.rename(columns={'customer_id_index': 'user_id','latitude':'latitude_user','longitude': 'longitude_user'}, inplace=True)\n",
    "df_users_test.info(memory_usage=\"deep\")\n",
    "# get all values of the column gender\n",
    "df_users_test['gender']."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyse des données Test Vendors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1672000 entries, 0 to 1671999\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count    Dtype         \n",
      "---  ------            --------------    -----         \n",
      " 0   customer_id       1672000 non-null  object        \n",
      " 1   gender            1134800 non-null  object        \n",
      " 2   status            1581400 non-null  float64       \n",
      " 3   verified          1581400 non-null  float64       \n",
      " 4   updated_at        1581400 non-null  datetime64[ns]\n",
      " 5   location_number   1672000 non-null  int64         \n",
      " 6   location_type     1672000 non-null  object        \n",
      " 7   latitude_user     1671700 non-null  float64       \n",
      " 8   longitude_user    1671700 non-null  float64       \n",
      " 9   vendor_id         1672000 non-null  int64         \n",
      " 10  latitude_vendor   1672000 non-null  float64       \n",
      " 11  longitude_vendor  1672000 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6), int64(2), object(3)\n",
      "memory usage: 394.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_client = pd.read_csv('./data/vendors.csv')\n",
    "df_client = df_client[['id', 'latitude', 'longitude']]\n",
    "df_client.rename(columns={'id': 'vendor_id','latitude': 'latitude_vendor', 'longitude': 'longitude_vendor'}, inplace=True)\n",
    "df_matrix_test = pd.merge(df_users_test, df_client, how='cross')\n",
    "#show the first 20 rows\n",
    "df_matrix_test.head(20)\n",
    "df_matrix_test.info(memory_usage=\"deep\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comme dit précédemment, nous allons utiliser la distance entre le client et le restaurant pour calculer la note de ce dernier. Nous allons donc calculer la distance entre le client et le restaurant via la latitude et la longitude de chacun."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#get deliverydistance and rating\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    # rayon de la terre en km\n",
    "    R = 6371\n",
    "\n",
    "    # conversion en radian\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    # formule\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * \\\n",
    "        np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    # distance en km\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "        CID X LOC_NUM X VENDOR  vendor    rating\n0              Z59FTQD X 0 X 4       4  4.051932\n1             Z59FTQD X 0 X 13      13  4.049152\n2             Z59FTQD X 0 X 20      20  4.047912\n3             Z59FTQD X 0 X 23      23  4.051857\n4             Z59FTQD X 0 X 28      28  4.026828\n...                        ...     ...       ...\n1671995      3O8LSR3 X 0 X 849     849  1.040217\n1671996      3O8LSR3 X 0 X 855     855  1.056561\n1671997      3O8LSR3 X 0 X 856     856  1.013522\n1671998      3O8LSR3 X 0 X 858     858  1.007700\n1671999      3O8LSR3 X 0 X 907     907  1.332086\n\n[1672000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CID X LOC_NUM X VENDOR</th>\n      <th>vendor</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Z59FTQD X 0 X 4</td>\n      <td>4</td>\n      <td>4.051932</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Z59FTQD X 0 X 13</td>\n      <td>13</td>\n      <td>4.049152</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Z59FTQD X 0 X 20</td>\n      <td>20</td>\n      <td>4.047912</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Z59FTQD X 0 X 23</td>\n      <td>23</td>\n      <td>4.051857</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Z59FTQD X 0 X 28</td>\n      <td>28</td>\n      <td>4.026828</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1671995</th>\n      <td>3O8LSR3 X 0 X 849</td>\n      <td>849</td>\n      <td>1.040217</td>\n    </tr>\n    <tr>\n      <th>1671996</th>\n      <td>3O8LSR3 X 0 X 855</td>\n      <td>855</td>\n      <td>1.056561</td>\n    </tr>\n    <tr>\n      <th>1671997</th>\n      <td>3O8LSR3 X 0 X 856</td>\n      <td>856</td>\n      <td>1.013522</td>\n    </tr>\n    <tr>\n      <th>1671998</th>\n      <td>3O8LSR3 X 0 X 858</td>\n      <td>858</td>\n      <td>1.007700</td>\n    </tr>\n    <tr>\n      <th>1671999</th>\n      <td>3O8LSR3 X 0 X 907</td>\n      <td>907</td>\n      <td>1.332086</td>\n    </tr>\n  </tbody>\n</table>\n<p>1672000 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matrix_test['deliverydistance'] = distance(df_matrix_test.latitude_user, df_matrix_test.longitude_user, df_matrix_test.latitude_vendor, df_matrix_test.longitude_vendor)\n",
    "df_matrix_test['rating'] = distance_to_rating(df_matrix_test.deliverydistance, df_matrix_test.deliverydistance.min(), df_matrix_test.deliverydistance.max(), min_rating, max_rating)\n",
    "df_matrix_test= df_matrix_test[['customer_id','location_number','vendor_id','rating']]\n",
    "df_matrix_test = df_matrix_test.assign(C=lambda x: x['customer_id'].astype(str) + ' X ' + x['location_number'].astype(str) + ' X ' + x['vendor_id'].astype(str))\n",
    "df_matrix_test.rename(columns={'C': 'CID X LOC_NUM X VENDOR', 'vendor_id': 'vendor'}, inplace=True)\n",
    "#remove duplicates\n",
    "df_matrix_test = df_matrix_test.drop_duplicates(subset=['CID X LOC_NUM X VENDOR'])\n",
    "df_matrix_test = df_matrix_test[['CID X LOC_NUM X VENDOR', 'vendor', 'rating']]\n",
    "df_matrix_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conversion de df_test en dataframe spark pour l'ALS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+------------------+\n",
      "|CID X LOC_NUM X VENDOR|vendor|            rating|\n",
      "+----------------------+------+------------------+\n",
      "|       Z59FTQD X 0 X 4|     4|4.0519319851434785|\n",
      "|      Z59FTQD X 0 X 13|    13| 4.049151517493291|\n",
      "|      Z59FTQD X 0 X 20|    20| 4.047911790961834|\n",
      "|      Z59FTQD X 0 X 23|    23| 4.051856681483615|\n",
      "|      Z59FTQD X 0 X 28|    28| 4.026827735173355|\n",
      "|      Z59FTQD X 0 X 33|    33| 4.049707754461085|\n",
      "|      Z59FTQD X 0 X 43|    43| 4.041168522886974|\n",
      "|      Z59FTQD X 0 X 44|    44|   4.0622632429566|\n",
      "|      Z59FTQD X 0 X 55|    55| 4.067821284898708|\n",
      "|      Z59FTQD X 0 X 66|    66| 4.026626544646668|\n",
      "|      Z59FTQD X 0 X 67|    67|4.0429185235927925|\n",
      "|      Z59FTQD X 0 X 75|    75| 4.052211766950357|\n",
      "|      Z59FTQD X 0 X 76|    76| 4.032150332283219|\n",
      "|      Z59FTQD X 0 X 78|    78| 4.052770278378749|\n",
      "|      Z59FTQD X 0 X 79|    79|4.0231979637790705|\n",
      "|      Z59FTQD X 0 X 81|    81| 4.056912698737247|\n",
      "|      Z59FTQD X 0 X 82|    82| 4.048137665162567|\n",
      "|      Z59FTQD X 0 X 83|    83| 4.061874648202295|\n",
      "|      Z59FTQD X 0 X 84|    84| 4.063916885611587|\n",
      "|      Z59FTQD X 0 X 85|    85| 4.052205902812711|\n",
      "+----------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark_2 = SparkSession.builder.appName(\"ALSMatrixFactorisationTEST\").config(\"spark.executor.memory\", \"16g\").config(\"spark.driver.memory\",\"16g\").getOrCreate()\n",
    "df_test = spark.createDataFrame(df_matrix_test)\n",
    "df_test.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+------------------+------------+----------------------------+\n",
      "|CID X LOC_NUM X VENDOR|vendor|            rating|vendor_index|CID X LOC_NUM X VENDOR_index|\n",
      "+----------------------+------+------------------+------------+----------------------------+\n",
      "|       Z59FTQD X 0 X 4|     4|4.0519319851434785|        59.0|                   1631859.0|\n",
      "|      Z59FTQD X 0 X 13|    13| 4.049151517493291|         6.0|                   1631806.0|\n",
      "|      Z59FTQD X 0 X 20|    20| 4.047911790961834|        27.0|                   1631827.0|\n",
      "|      Z59FTQD X 0 X 23|    23| 4.051856681483615|        34.0|                   1631834.0|\n",
      "|      Z59FTQD X 0 X 28|    28| 4.026827735173355|        43.0|                   1631843.0|\n",
      "|      Z59FTQD X 0 X 33|    33| 4.049707754461085|        54.0|                   1631854.0|\n",
      "|      Z59FTQD X 0 X 43|    43| 4.041168522886974|        62.0|                   1631862.0|\n",
      "|      Z59FTQD X 0 X 44|    44|   4.0622632429566|        63.0|                   1631863.0|\n",
      "|      Z59FTQD X 0 X 55|    55| 4.067821284898708|        67.0|                   1631867.0|\n",
      "|      Z59FTQD X 0 X 66|    66| 4.026626544646668|        74.0|                   1631874.0|\n",
      "|      Z59FTQD X 0 X 67|    67|4.0429185235927925|        75.0|                   1631875.0|\n",
      "|      Z59FTQD X 0 X 75|    75| 4.052211766950357|        79.0|                   1631879.0|\n",
      "|      Z59FTQD X 0 X 76|    76| 4.032150332283219|        80.0|                   1631880.0|\n",
      "|      Z59FTQD X 0 X 78|    78| 4.052770278378749|        81.0|                   1631881.0|\n",
      "|      Z59FTQD X 0 X 79|    79|4.0231979637790705|        82.0|                   1631882.0|\n",
      "|      Z59FTQD X 0 X 81|    81| 4.056912698737247|        83.0|                   1631883.0|\n",
      "|      Z59FTQD X 0 X 82|    82| 4.048137665162567|        84.0|                   1631884.0|\n",
      "|      Z59FTQD X 0 X 83|    83| 4.061874648202295|        85.0|                   1631885.0|\n",
      "|      Z59FTQD X 0 X 84|    84| 4.063916885611587|        86.0|                   1631886.0|\n",
      "|      Z59FTQD X 0 X 85|    85| 4.052205902812711|        92.0|                   1631892.0|\n",
      "+----------------------+------+------------------+------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(df_test.columns)-set(['rating'])) ]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "df_test = pipeline.fit(df_test).transform(df_test)\n",
    "df_test.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Création du modèle ALS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    maxIter=5,\n",
    "    regParam=0.09,\n",
    "    rank=25,\n",
    "    userCol=\"CID X LOC_NUM X VENDOR_index\",\n",
    "    itemCol=\"vendor_index\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    ")\n",
    "model = als.fit(df_training)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation du modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.631113253552553\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_test)\n",
    "predictions = predictions.na.fill({'prediction': 0})\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE=\" + str(rmse))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recommandation pour tous les utilisateurs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------+\n",
      "|CID X LOC_NUM X VENDOR_index|     recommendations|\n",
      "+----------------------------+--------------------+\n",
      "|                          31|[{90, 1.4472833},...|\n",
      "|                          34|[{5, 2.9394953}, ...|\n",
      "|                          53|[{34, 2.1476676},...|\n",
      "|                          65|[{62, 2.7835677},...|\n",
      "|                          78|[{4, 2.6957803}, ...|\n",
      "|                          85|[{6, 2.6413693}, ...|\n",
      "|                         108|[{6, 1.2538116}, ...|\n",
      "|                         133|[{80, 0.9401024},...|\n",
      "|                         137|[{23, 0.89890945}...|\n",
      "|                         148|[{91, 2.1176667},...|\n",
      "|                         155|[{4, 2.348916}, {...|\n",
      "|                         193|[{15, 0.90692633}...|\n",
      "|                         211|[{86, 2.1137369},...|\n",
      "|                         243|[{75, 0.9059224},...|\n",
      "|                         251|[{99, 2.1829667},...|\n",
      "|                         255|[{61, 0.89535207}...|\n",
      "|                         296|[{87, 0.9130581},...|\n",
      "|                         321|[{0, 2.137539}, {...|\n",
      "|                         322|[{1, 0.90986335},...|\n",
      "|                         362|[{39, 0.88638926}...|\n",
      "+----------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate top 20 vendor recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(20).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Résultats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# get f1 score\n",
    "def get_scores(df):\n",
    "    df = df.withColumn(\"TP\", when((col(\"rating\") >= 4) & (col(\"prediction\") >= 4), 1).otherwise(0))\n",
    "    df = df.withColumn(\"FP\", when((col(\"rating\") < 4) & (col(\"prediction\") >= 4), 1).otherwise(0))\n",
    "    df = df.withColumn(\"FN\", when((col(\"rating\") >= 4) & (col(\"prediction\") < 4), 1).otherwise(0))\n",
    "    df = df.withColumn(\"TN\", when((col(\"rating\") < 4) & (col(\"prediction\") < 4), 1).otherwise(0))\n",
    "    TP = df.agg(sum(\"TP\")).collect()[0][0]\n",
    "    FP = df.agg(sum(\"FP\")).collect()[0][0]\n",
    "    FN = df.agg(sum(\"FN\")).collect()[0][0]\n",
    "    TN = df.agg(sum(\"TN\")).collect()[0][0]\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    return f1_score, accuracy, precision, recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mget_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[25], line 11\u001B[0m, in \u001B[0;36mget_scores\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m      9\u001B[0m FN \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39magg(\u001B[38;5;28msum\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFN\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     10\u001B[0m TN \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39magg(\u001B[38;5;28msum\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTN\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 11\u001B[0m precision \u001B[38;5;241m=\u001B[39m \u001B[43mTP\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mTP\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mFP\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m recall \u001B[38;5;241m=\u001B[39m TP \u001B[38;5;241m/\u001B[39m (TP \u001B[38;5;241m+\u001B[39m FN)\n\u001B[0;32m     13\u001B[0m f1_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m (precision \u001B[38;5;241m*\u001B[39m recall) \u001B[38;5;241m/\u001B[39m (precision \u001B[38;5;241m+\u001B[39m recall)\n",
      "\u001B[1;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "scores = get_scores(predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"F1 score: \", scores[0], \"\\nAccuracy: \", scores[1], \"\\nPrecision: \", scores[2], \"\\nRecall: \", scores[3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Par les scores obtenus, on peut dire que le modèle n'est pas très performant. Cela est dû au fait que les données sont très déséquilibrées. En effet, il y a beaucoup plus de ratings négatifs que de ratings positifs. Il faudrait donc équilibrer les données pour avoir un modèle plus performant.\n",
    "Par ailleurs, on aurait pu changer la méthode pour noter les ratings. En effet, on a choisi de noter les ratings en fonction de la distance entre le client et le vendeur. On aurait pu choisir de noter les ratings en fonction du temps de livraison, de la qualité de la nourriture, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a data frame with CID X LOC_NUM X VENDOR, and target (0,1)\n",
    "# target = 1 if vendor is in the top 20 recommendations\n",
    "\n",
    "# get top 20 recommendations for each user\n",
    "top_20 = model.recommendForAllUsers(20)\n",
    "\n",
    "# explode the recommendations\n",
    "top_20 = top_20.withColumn(\"recommendations\", explode(\"recommendations\"))\n",
    "\n",
    "# get the vendor_id from the recommendations\n",
    "top_20 = top_20.withColumn(\"vendor_id\", top_20[\"recommendations\"][\"vendor_index\"])\n",
    "\n",
    "# get the rating from the recommendations\n",
    "top_20 = top_20.withColumn(\"rating\", top_20[\"recommendations\"][\"rating\"])\n",
    "\n",
    "# drop the recommendations column\n",
    "top_20 = top_20.drop(\"recommendations\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour ce qui est de la création du dataframe avec les targets, on a choisi de prendre les 20 premières recommandations pour chaque utilisateur. On a ensuite créé une colonne target qui vaut 1 si le vendeur est dans les 20 premières recommandations et 0 sinon."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# create a list of top 20 vendors for each user\n",
    "top_20 = top_20.groupBy(\"CID X LOC_NUM X VENDOR_index\").agg(collect_list(\"vendor_id\").alias(\"top_20\"))\n",
    "top_20 = top_20.withColumn(\"top_20\", top_20[\"top_20\"].cast(ArrayType(IntegerType())))\n",
    "top_20.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create column target on df_test with value 1 if vendor is in the top 20 recommendations\n",
    "df_test = df_test.join(top_20, \"CID X LOC_NUM X VENDOR_index\", \"left\")\n",
    "df_test = df_test.withColumn(\"target\", when(array_contains(\"top_20\", col(\"vendor_index\")), 1).otherwise(0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a data frame with CID X LOC_NUM X VENDOR and target and drop duplicates\n",
    "df_test = df_test.select(\"CID X LOC_NUM X VENDOR\", \"target\").dropDuplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a submission.csv file with the target\n",
    "df_submission = df_test.toPandas()\n",
    "df_submission.info()\n",
    "df_submission.sort_values(by=['CID X LOC_NUM X VENDOR'], inplace=True)\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
